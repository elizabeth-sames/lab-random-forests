{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a2a60",
   "metadata": {},
   "source": [
    "Apply the Random Forests algorithm but this time only by upscaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa782cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = pd.read_csv('files_for_lab/categorical.csv')\n",
    "numerical = pd.read_csv('files_for_lab/numerical.csv')\n",
    "target = pd.read_csv('files_for_lab/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the categoricals\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first').fit(categorical)\n",
    "encoded_categorical = encoder.transform(categorical).toarray()\n",
    "encoded_categorical = pd.DataFrame(encoded_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21300b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([encoded_categorical, numerical, target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a19ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TARGET_D'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ed2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.TARGET_B.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73520eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsample\n",
    "from sklearn.utils import resample\n",
    "majority_class = data[data.TARGET_B == 0]\n",
    "minority_class = data[data.TARGET_B == 1]\n",
    "\n",
    "minority_class_upsampled = resample(minority_class, \n",
    "                                   replace=True, \n",
    "                                   n_samples = len(majority_class))\n",
    "\n",
    "data_upsampled = pd.concat([majority_class, minority_class_upsampled], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e50cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X-y split\n",
    "y = data_upsampled['TARGET_B']\n",
    "X = data_upsampled.drop(['TARGET_B'], axis=1)\n",
    "\n",
    "#train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "y_train_regression = X_train['TARGET_D']\n",
    "y_test_regression = X_test['TARGET_D']\n",
    "\n",
    "\n",
    "X_train = X_train.drop(['TARGET_D'], axis = 1)\n",
    "X_test = X_test.drop(['TARGET_D'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the Random Forest classification model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = RandomForestClassifier(max_depth=5,\n",
    "                             min_samples_split=20,\n",
    "                             min_samples_leaf =20)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('train score: ',clf.score(X_train, y_train))\n",
    "print('test score: ',clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation score\n",
    "cross_val_scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "print(np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983f8ff",
   "metadata": {},
   "source": [
    "Discuss the output and its impact in the business scenario. Is the cost of a false positive equals to the cost of the false negative? How would you change your algorithm or data in order to maximize the return of the business?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae61d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aee1f7",
   "metadata": {},
   "source": [
    "The false positives are people that were predicted to donate but did not donate, and the false negatives are the people that were predicted to be non-donors but did actually donate. \n",
    "\n",
    "In this case if we only send the promotional mail to the predicted donors the cost of a false negative(missing out on a donation) is far more than the cost of a false positive(the cost of postage). \n",
    "\n",
    "It would be best to focus on minimizing the false negatives in order to increase total donation amount. We can lower the threshold for a positive prediction in order to increase the amount of positives predicted. The optimal donor probability threshold for maximum revenue would depend on the cost of postage of each promotional mail(not included in the case study) and the estimated donation amounts(which I will build a model to predict in the next lab).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adcf73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding predictions to dataset\n",
    "y = X['TARGET_D']\n",
    "X = X.drop(['TARGET_D'], axis=1)\n",
    "\n",
    "pred = clf.predict(X)\n",
    "pred_prob = clf.predict_proba(X)\n",
    "\n",
    "X['predicted_donor'] = pred\n",
    "X['predicted_donor_prob'] = [pred_prob[i][1] for i in range(len(pred_prob))]\n",
    "donor_prediction_data = X.copy()\n",
    "donor_prediction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find threshold to optimize donations\n",
    "best_threshold = 0\n",
    "highest_revenue = 0\n",
    "\n",
    "for i in range(30,50):\n",
    "    d = donor_prediction_data[donor_prediction_data['predicted_donor_prob']>=(i/100)]\n",
    "    cost = .68*len(d)\n",
    "    donations = d['TARGET_D'].sum()\n",
    "    revenue = donations - cost\n",
    "    if revenue > highest_revenue:\n",
    "        highest_revenue = revenue\n",
    "        best_threshold = i/100\n",
    "    \n",
    "print('highest revenue: ', highest_revenue, ' best threshold: ', best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564133dd",
   "metadata": {},
   "source": [
    "For the current dataset, if we assume that each piece of promotional mail cost 68 cents(I did not see this in the case study documentation but was mentioned in class) and all 95412 potential donors were to receive one, that would cost the company $64880.16. The total amount of donations received was $75668.70, leaving $10788.54 in revenue.\n",
    "\n",
    "But instead if they had only sent it to the potential donors that the model predicted at least .34 probability of donating, the revenue would have increased to $1284838.32."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
